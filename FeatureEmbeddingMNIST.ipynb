{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a78b5-c175-4605-a663-5e9a5caab969",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea37aa-d585-4299-a290-cf30df8ea635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "cuda = torch.cuda.is_available()\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5de533-3885-462b-8c66-2edfab511d7e",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "We'll go through learning feature embeddings using different loss functions on MNIST dataset. This is just for visualization purposes, thus we'll be using 2-dimensional embeddings which isn't the best choice in practice.\n",
    "\n",
    "For every experiment the same embedding network is used (32 conv 5x5 -> PReLU -> MaxPool 2x2 -> 64 conv 5x5 -> PReLU -> MaxPool 2x2 -> Dense 256 -> PReLU -> Dense 256 -> PReLU -> Dense 2) and we don't do any hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89348e4-b038-4669-9af4-80a114ba399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = 0.28604059698879553, 0.35302424451492237\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = FashionMNIST('../data/FashionMNIST', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((mean,), (std,))\n",
    "                             ]))\n",
    "test_dataset = FashionMNIST('../data/FashionMNIST', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean,), (std,))\n",
    "                            ]))\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de91f4-709a-4a42-a3eb-9fe405ca3df0",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829ed96-9686-459b-bc16-bae423aa8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample creates randomly a positive or a negative pair\n",
    "    Test: Creates fixed pairs for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "\n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.train_labels\n",
    "            self.train_data = self.mnist_dataset.train_data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "        else:\n",
    "            # generate fixed pairs for testing\n",
    "            self.test_labels = self.mnist_dataset.test_labels\n",
    "            self.test_data = self.mnist_dataset.test_data\n",
    "            self.labels_set = set(self.test_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            positive_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                               1]\n",
    "                              for i in range(0, len(self.test_data), 2)]\n",
    "\n",
    "            negative_pairs = [[i,\n",
    "                               random_state.choice(self.label_to_indices[\n",
    "                                                       np.random.choice(\n",
    "                                                           list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                       )\n",
    "                                                   ]),\n",
    "                               0]\n",
    "                              for i in range(1, len(self.test_data), 2)]\n",
    "            self.test_pairs = positive_pairs + negative_pairs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            target = np.random.randint(0, 2)\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            if target == 1:\n",
    "                siamese_index = index\n",
    "                while siamese_index == index:\n",
    "                    siamese_index = np.random.choice(self.label_to_indices[label1])\n",
    "            else:\n",
    "                siamese_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "                siamese_index = np.random.choice(self.label_to_indices[siamese_label])\n",
    "            img2 = self.train_data[siamese_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_pairs[index][0]]\n",
    "            img2 = self.test_data[self.test_pairs[index][1]]\n",
    "            target = self.test_pairs[index][2]\n",
    "\n",
    "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        return (img1, img2), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "\n",
    "class TripletMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Train: For each sample (anchor) randomly chooses a positive and negative samples\n",
    "    Test: Creates fixed triplets for testing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.train_labels\n",
    "            self.train_data = self.mnist_dataset.train_data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "        else:\n",
    "            self.test_labels = self.mnist_dataset.test_labels\n",
    "            self.test_data = self.mnist_dataset.test_data\n",
    "            # generate fixed triplets for testing\n",
    "            self.labels_set = set(self.test_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            triplets = [[i,\n",
    "                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                         random_state.choice(self.label_to_indices[\n",
    "                                                 np.random.choice(\n",
    "                                                     list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                 )\n",
    "                                             ])\n",
    "                         ]\n",
    "                        for i in range(len(self.test_data))]\n",
    "            self.test_triplets = triplets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            positive_index = index\n",
    "            while positive_index == index:\n",
    "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
    "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
    "            img2 = self.train_data[positive_index]\n",
    "            img3 = self.train_data[negative_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_triplets[index][0]]\n",
    "            img2 = self.test_data[self.test_triplets[index][1]]\n",
    "            img3 = self.test_data[self.test_triplets[index][2]]\n",
    "\n",
    "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        img3 = Image.fromarray(img3.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            img3 = self.transform(img3)\n",
    "        return (img1, img2, img3), []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "\n",
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34737564-7770-4c86-85f4-b507812c6ec0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3b56b-20a0-4b40-adf1-61a347f9e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],\n",
    "        start_epoch=0):\n",
    "    \"\"\"\n",
    "    Loaders, model, loss function and metrics should work together for a given task,\n",
    "    i.e. The model should be able to process data output of loaders,\n",
    "    loss function should process target output of loaders and outputs from the model\n",
    "\n",
    "    Examples: Classification: batch loader, classification model, NLL loss, accuracy metric\n",
    "    Siamese network: Siamese loader, siamese model, contrastive loss\n",
    "    Online triplet learning: batch loader, embedding model, online triplet loss\n",
    "    \"\"\"\n",
    "    for epoch in range(0, start_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        scheduler.step()\n",
    "\n",
    "        # Train stage\n",
    "        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\n",
    "\n",
    "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "        val_loss, metrics = test_epoch(val_loader, model, loss_fn, cuda, metrics)\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        message += '\\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,\n",
    "                                                                                 val_loss)\n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "        print(message)\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = target if len(target) > 0 else None\n",
    "        if not type(data) in (tuple, list):\n",
    "            data = (data,)\n",
    "        if cuda:\n",
    "            data = tuple(d.cuda() for d in data)\n",
    "            if target is not None:\n",
    "                target = target.cuda()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*data)\n",
    "\n",
    "        if type(outputs) not in (tuple, list):\n",
    "            outputs = (outputs,)\n",
    "\n",
    "        loss_inputs = outputs\n",
    "        if target is not None:\n",
    "            target = (target,)\n",
    "            loss_inputs += target\n",
    "\n",
    "        loss_outputs = loss_fn(*loss_inputs)\n",
    "        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric(outputs, target, loss_outputs)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx * len(data[0]), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
    "            for metric in metrics:\n",
    "                message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "            print(message)\n",
    "            losses = []\n",
    "\n",
    "    total_loss /= (batch_idx + 1)\n",
    "    return total_loss, metrics\n",
    "\n",
    "\n",
    "def test_epoch(val_loader, model, loss_fn, cuda, metrics):\n",
    "    with torch.no_grad():\n",
    "        for metric in metrics:\n",
    "            metric.reset()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            target = target if len(target) > 0 else None\n",
    "            if not type(data) in (tuple, list):\n",
    "                data = (data,)\n",
    "            if cuda:\n",
    "                data = tuple(d.cuda() for d in data)\n",
    "                if target is not None:\n",
    "                    target = target.cuda()\n",
    "\n",
    "            outputs = model(*data)\n",
    "\n",
    "            if type(outputs) not in (tuple, list):\n",
    "                outputs = (outputs,)\n",
    "            loss_inputs = outputs\n",
    "            if target is not None:\n",
    "                target = (target,)\n",
    "                loss_inputs += target\n",
    "\n",
    "            loss_outputs = loss_fn(*loss_inputs)\n",
    "            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric(outputs, target, loss_outputs)\n",
    "\n",
    "    return val_loss, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8f118-11e6-4ce1-920c-e63cd1337d53",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aef810-919c-4602-bf4b-0462a1ee0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
    "              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n",
    "              '#bcbd22', '#17becf']\n",
    "\n",
    "def plot_embeddings(embeddings, targets, xlim=None, ylim=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(10):\n",
    "        inds = np.where(targets==i)[0]\n",
    "        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])\n",
    "    if xlim:\n",
    "        plt.xlim(xlim[0], xlim[1])\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0], ylim[1])\n",
    "    plt.legend(mnist_classes)\n",
    "\n",
    "def extract_embeddings(dataloader, model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        embeddings = np.zeros((len(dataloader.dataset), 2))\n",
    "        labels = np.zeros(len(dataloader.dataset))\n",
    "        k = 0\n",
    "        for images, target in dataloader:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()\n",
    "            labels[k:k+len(images)] = target.numpy()\n",
    "            k += len(images)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e541749-4c7d-4280-8344-8dc51e0a62c4",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205ff9f-bae7-4865-b42d-6e277cb31312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(vectors):\n",
    "    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n",
    "        dim=1).view(-1, 1)\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "class PairSelector:\n",
    "    \"\"\"\n",
    "    Implementation should return indices of positive pairs and negative pairs that will be passed to compute\n",
    "    Contrastive Loss\n",
    "    return positive_pairs, negative_pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_pairs(self, embeddings, labels):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AllPositivePairSelector(PairSelector):\n",
    "    \"\"\"\n",
    "    Discards embeddings and generates all possible pairs given labels.\n",
    "    If balance is True, negative pairs are a random sample to match the number of positive samples\n",
    "    \"\"\"\n",
    "    def __init__(self, balance=True):\n",
    "        super(AllPositivePairSelector, self).__init__()\n",
    "        self.balance = balance\n",
    "\n",
    "    def get_pairs(self, embeddings, labels):\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        all_pairs = np.array(list(combinations(range(len(labels)), 2)))\n",
    "        all_pairs = torch.LongTensor(all_pairs)\n",
    "        positive_pairs = all_pairs[(labels[all_pairs[:, 0]] == labels[all_pairs[:, 1]]).nonzero()]\n",
    "        negative_pairs = all_pairs[(labels[all_pairs[:, 0]] != labels[all_pairs[:, 1]]).nonzero()]\n",
    "        if self.balance:\n",
    "            negative_pairs = negative_pairs[torch.randperm(len(negative_pairs))[:len(positive_pairs)]]\n",
    "\n",
    "        return positive_pairs, negative_pairs\n",
    "\n",
    "\n",
    "class HardNegativePairSelector(PairSelector):\n",
    "    \"\"\"\n",
    "    Creates all possible positive pairs. For negative pairs, pairs with smallest distance are taken into consideration,\n",
    "    matching the number of positive pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cpu=True):\n",
    "        super(HardNegativePairSelector, self).__init__()\n",
    "        self.cpu = cpu\n",
    "\n",
    "    def get_pairs(self, embeddings, labels):\n",
    "        if self.cpu:\n",
    "            embeddings = embeddings.cpu()\n",
    "        distance_matrix = pdist(embeddings)\n",
    "\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        all_pairs = np.array(list(combinations(range(len(labels)), 2)))\n",
    "        all_pairs = torch.LongTensor(all_pairs)\n",
    "        positive_pairs = all_pairs[(labels[all_pairs[:, 0]] == labels[all_pairs[:, 1]]).nonzero()]\n",
    "        negative_pairs = all_pairs[(labels[all_pairs[:, 0]] != labels[all_pairs[:, 1]]).nonzero()]\n",
    "\n",
    "        negative_distances = distance_matrix[negative_pairs[:, 0], negative_pairs[:, 1]]\n",
    "        negative_distances = negative_distances.cpu().data.numpy()\n",
    "        top_negatives = np.argpartition(negative_distances, len(positive_pairs))[:len(positive_pairs)]\n",
    "        top_negative_pairs = negative_pairs[torch.LongTensor(top_negatives)]\n",
    "\n",
    "        return positive_pairs, top_negative_pairs\n",
    "\n",
    "\n",
    "class TripletSelector:\n",
    "    \"\"\"\n",
    "    Implementation should return indices of anchors, positive and negative samples\n",
    "    return np array of shape [N_triplets x 3]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AllTripletSelector(TripletSelector):\n",
    "    \"\"\"\n",
    "    Returns all possible triplets\n",
    "    May be impractical in most cases\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AllTripletSelector, self).__init__()\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "\n",
    "            # Add all negatives for all positive pairs\n",
    "            temp_triplets = [[anchor_positive[0], anchor_positive[1], neg_ind] for anchor_positive in anchor_positives\n",
    "                             for neg_ind in negative_indices]\n",
    "            triplets += temp_triplets\n",
    "\n",
    "        return torch.LongTensor(np.array(triplets))\n",
    "\n",
    "\n",
    "def hardest_negative(loss_values):\n",
    "    hard_negative = np.argmax(loss_values)\n",
    "    return hard_negative if loss_values[hard_negative] > 0 else None\n",
    "\n",
    "\n",
    "def random_hard_negative(loss_values):\n",
    "    hard_negatives = np.where(loss_values > 0)[0]\n",
    "    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n",
    "\n",
    "\n",
    "def semihard_negative(loss_values, margin):\n",
    "    semihard_negatives = np.where(np.logical_and(loss_values < margin, loss_values > 0))[0]\n",
    "    return np.random.choice(semihard_negatives) if len(semihard_negatives) > 0 else None\n",
    "\n",
    "\n",
    "class FunctionNegativeTripletSelector(TripletSelector):\n",
    "    \"\"\"\n",
    "    For each positive pair, takes the hardest negative sample (with the greatest triplet loss value) to create a triplet\n",
    "    Margin should match the margin used in triplet loss.\n",
    "    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n",
    "    and return a negative index for that pair\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, negative_selection_fn, cpu=True):\n",
    "        super(FunctionNegativeTripletSelector, self).__init__()\n",
    "        self.cpu = cpu\n",
    "        self.margin = margin\n",
    "        self.negative_selection_fn = negative_selection_fn\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        if self.cpu:\n",
    "            embeddings = embeddings.cpu()\n",
    "        distance_matrix = pdist(embeddings)\n",
    "        distance_matrix = distance_matrix.cpu()\n",
    "\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "            anchor_positives = np.array(anchor_positives)\n",
    "\n",
    "            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n",
    "            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n",
    "                loss_values = ap_distance - distance_matrix[torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n",
    "                loss_values = loss_values.data.cpu().numpy()\n",
    "                hard_negative = self.negative_selection_fn(loss_values)\n",
    "                if hard_negative is not None:\n",
    "                    hard_negative = negative_indices[hard_negative]\n",
    "                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n",
    "\n",
    "        if len(triplets) == 0:\n",
    "            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n",
    "\n",
    "        triplets = np.array(triplets)\n",
    "\n",
    "        return torch.LongTensor(triplets)\n",
    "\n",
    "\n",
    "def HardestNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n",
    "                                                                                 negative_selection_fn=hardest_negative,\n",
    "                                                                                 cpu=cpu)\n",
    "\n",
    "\n",
    "def RandomNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n",
    "                                                                                negative_selection_fn=random_hard_negative,\n",
    "                                                                                cpu=cpu)\n",
    "\n",
    "\n",
    "def SemihardNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n",
    "                                                                                  negative_selection_fn=lambda x: semihard_negative(x, margin),\n",
    "                                                                                  cpu=cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97873995-29ed-4b13-b3d0-3e3d3e9f9fae",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495a680-bf7b-4d6a-b151-969e8819ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 2)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.convnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "class EmbeddingNetL2(EmbeddingNet):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNetL2, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = super(EmbeddingNetL2, self).forward(x)\n",
    "        output /= output.pow(2).sum(1, keepdim=True).sqrt()\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "\n",
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self, embedding_net, n_classes):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.n_classes = n_classes\n",
    "        self.nonlinear = nn.PReLU()\n",
    "        self.fc1 = nn.Linear(2, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.embedding_net(x)\n",
    "        output = self.nonlinear(output)\n",
    "        scores = F.log_softmax(self.fc1(output), dim=-1)\n",
    "        return scores\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.nonlinear(self.embedding_net(x))\n",
    "\n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        output1 = self.embedding_net(x1)\n",
    "        output2 = self.embedding_net(x2)\n",
    "        return output1, output2\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "\n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.embedding_net(x1)\n",
    "        output2 = self.embedding_net(x2)\n",
    "        output3 = self.embedding_net(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66c118-aad4-4653-b6ae-61c887cab3e9",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359b70b-c8c0-4a49-880d-a9a3dc58c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target, size_average=True):\n",
    "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
    "        losses = 0.5 * (target.float() * distances +\n",
    "                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
    "        return losses.mean() if size_average else losses.sum()\n",
    "\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()\n",
    "\n",
    "\n",
    "class OnlineContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Online Contrastive loss\n",
    "    Takes a batch of embeddings and corresponding labels.\n",
    "    Pairs are generated using pair_selector object that take embeddings and targets and return indices of positive\n",
    "    and negative pairs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, pair_selector):\n",
    "        super(OnlineContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pair_selector = pair_selector\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "        positive_pairs, negative_pairs = self.pair_selector.get_pairs(embeddings, target)\n",
    "        if embeddings.is_cuda:\n",
    "            positive_pairs = positive_pairs.cuda()\n",
    "            negative_pairs = negative_pairs.cuda()\n",
    "        positive_loss = (embeddings[positive_pairs[:, 0]] - embeddings[positive_pairs[:, 1]]).pow(2).sum(1)\n",
    "        negative_loss = F.relu(\n",
    "            self.margin - (embeddings[negative_pairs[:, 0]] - embeddings[negative_pairs[:, 1]]).pow(2).sum(\n",
    "                1).sqrt()).pow(2)\n",
    "        loss = torch.cat([positive_loss, negative_loss], dim=0)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class OnlineTripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Online Triplets loss\n",
    "    Takes a batch of embeddings and corresponding labels.\n",
    "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
    "    triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, triplet_selector):\n",
    "        super(OnlineTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.triplet_selector = triplet_selector\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "\n",
    "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
    "\n",
    "        if embeddings.is_cuda:\n",
    "            triplets = triplets.cuda()\n",
    "\n",
    "        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n",
    "        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
    "\n",
    "        return losses.mean(), len(triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401f34e-064d-4b6f-8f96-aced2212e87c",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f8980-a0c7-48ed-944a-44cdd70eec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, outputs, target, loss):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def value(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AccumulatedAccuracyMetric(Metric):\n",
    "    \"\"\"\n",
    "    Works with classification model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def __call__(self, outputs, target, loss):\n",
    "        pred = outputs[0].data.max(1, keepdim=True)[1]\n",
    "        self.correct += pred.eq(target[0].data.view_as(pred)).cpu().sum()\n",
    "        self.total += target[0].size(0)\n",
    "        return self.value()\n",
    "\n",
    "    def reset(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def value(self):\n",
    "        return 100 * float(self.correct) / self.total\n",
    "\n",
    "    def name(self):\n",
    "        return 'Accuracy'\n",
    "\n",
    "\n",
    "class AverageNonzeroTripletsMetric(Metric):\n",
    "    '''\n",
    "    Counts average number of nonzero triplets found in minibatches\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.values = []\n",
    "\n",
    "    def __call__(self, outputs, target, loss):\n",
    "        self.values.append(loss[1])\n",
    "        return self.value()\n",
    "\n",
    "    def reset(self):\n",
    "        self.values = []\n",
    "\n",
    "    def value(self):\n",
    "        return np.mean(self.values)\n",
    "\n",
    "    def name(self):\n",
    "        return 'Average nonzero triplets'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558a8a8-7cb9-402d-aa29-428f1548ce54",
   "metadata": {},
   "source": [
    "# Baseline: Classification with softmax\n",
    "We'll train the model for classification and use outputs of penultimate layer as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d73567-0f50-4162-a3ab-c383d4cadf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "batch_size = 256\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "embedding_net = EmbeddingNet()\n",
    "model = ClassificationNet(embedding_net, n_classes=n_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df1161-3991-428d-abb7-f706a459f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(train_loader, test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AccumulatedAccuracyMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ac7fd-0826-4007-8fae-0e0cbeeec7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_baseline, train_labels_baseline = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_baseline, train_labels_baseline)\n",
    "val_embeddings_baseline, val_labels_baseline = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_baseline, val_labels_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc35cd8-521e-44bb-bd33-af59367208da",
   "metadata": {},
   "source": [
    "# Siamese network\n",
    "We'll train a siamese network that takes a pair of images and trains the embeddings so that the distance between them is minimized if their from the same class or greater than some margin value if they represent different classes.\n",
    "We'll minimize a contrastive loss function*:\n",
    "$$L_{contrastive}(x_0, x_1, y) = \\frac{1}{2} y \\lVert f(x_0)-f(x_1)\\rVert_2^2 + \\frac{1}{2}(1-y)\\{max(0, m-\\lVert f(x_0)-f(x_1)\\rVert_2)\\}^2$$\n",
    "\n",
    "*Raia Hadsell, Sumit Chopra, Yann LeCun, [Dimensionality reduction by learning an invariant mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf), CVPR 2006*\n",
    "\n",
    "## Steps\n",
    "1. Create a dataset returning pairs - **SiameseMNIST** class from *datasets.py*, wrapper for MNIST-like classes.\n",
    "2. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
    "3. Define **siamese** network processing pairs of inputs - **SiameseNet** wrapping *EmbeddingNet*\n",
    "4. Train the network with **ContrastiveLoss** - *losses.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffdac3-39e3-4d9e-a9b7-a072ce7b29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "siamese_train_dataset = SiameseMNIST(train_dataset) # Returns pairs of images and target same/different\n",
    "siamese_test_dataset = SiameseMNIST(test_dataset)\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "siamese_train_loader = torch.utils.data.DataLoader(siamese_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "siamese_test_loader = torch.utils.data.DataLoader(siamese_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "# Step 2\n",
    "embedding_net = EmbeddingNet()\n",
    "# Step 3\n",
    "model = SiameseNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "# Step 4\n",
    "margin = 1.\n",
    "loss_fn = ContrastiveLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99165c1e-e044-47ce-b04f-0fa166088e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(siamese_train_loader, siamese_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62d707-73df-4a4f-ac94-4fcbfed3af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_cl, train_labels_cl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_cl, train_labels_cl)\n",
    "val_embeddings_cl, val_labels_cl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_cl, val_labels_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2908d9f-3330-4979-b92b-60e1296f46b6",
   "metadata": {},
   "source": [
    "# Triplet network\n",
    "We'll train a triplet network, that takes an anchor, positive (same class as anchor) and negative (different class than anchor) examples. The objective is to learn embeddings such that the anchor is closer to the positive example than it is to the negative example by some margin value.\n",
    "\n",
    "![alt text](images/anchor_negative_positive.png \"Source: FaceNet\")\n",
    "Source: [2] *Schroff, Florian, Dmitry Kalenichenko, and James Philbin. [Facenet: A unified embedding for face recognition and clustering.](https://arxiv.org/abs/1503.03832) CVPR 2015.*\n",
    "\n",
    "**Triplet loss**:   $L_{triplet}(x_a, x_p, x_n) = max(0, m +  \\lVert f(x_a)-f(x_p)\\rVert_2^2 - \\lVert f(x_a)-f(x_n)\\rVert_2^2$\\)\n",
    "\n",
    "## Steps\n",
    "1. Create a dataset returning triplets - **TripletMNIST** class from *datasets.py*, wrapper for MNIST-like classes\n",
    "2. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
    "3. Define **triplet** network processing triplets - **TripletNet** wrapping *EmbeddingNet*\n",
    "4. Train the network with **TripletLoss** - *losses.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912b080-74d2-42de-bcd4-9e0e302749e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_train_dataset = TripletMNIST(train_dataset) # Returns triplets of images\n",
    "triplet_test_dataset = TripletMNIST(test_dataset)\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = TripletNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = TripletLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5564bd3e-9f2f-4b5e-b9b2-8aea0e68cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc45f40-4887-439b-91a7-21a6d0a4b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_tl, train_labels_tl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_tl, train_labels_tl)\n",
    "val_embeddings_tl, val_labels_tl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_tl, val_labels_tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad794b74-63eb-4daa-a1c1-055d6dde2e05",
   "metadata": {},
   "source": [
    "# Online pair/triplet selection - negative mining\n",
    "There are couple of problems with siamese and triplet networks.\n",
    "1. The number of possible pairs/triplets grows **quadratically/cubically** with the number of examples. It's infeasible to process them all\n",
    "2. We generate pairs/triplets randomly. As the training continues, more and more pairs/triplets are easy to deal with (their loss value is very small or even 0), preventing the network from training. We need to provide the network with **hard examples**.\n",
    "3. Each image that is fed to the network is used only for computation of contrastive/triplet loss for only one pair/triplet. The computation is somewhat wasted; once the embedding is computed, it could be reused for many pairs/triplets.\n",
    "\n",
    "To deal with that efficiently, we'll feed a network with standard mini-batches as we did for classification. The loss function will be responsible for selection of hard pairs and triplets within mini-batch. In these case, if we feed the network with 16 images per 10 classes, we can process up to $159*160/2 = 12720$ pairs and $10*16*15/2*(9*16) = 172800$ triplets, compared to 80 pairs and 53 triplets in previous implementation.\n",
    "\n",
    "We can find some strategies on how to select triplets in [2] and [3] *Alexander Hermans, Lucas Beyer, Bastian Leibe, [In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/pdf/1703.07737), 2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea99fb-d174-4f29-b52c-842db0930fc2",
   "metadata": {},
   "source": [
    "## Online pair selection\n",
    "## Steps\n",
    "1. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples *datasets.py*\n",
    "2. Create data loaders with the batch sampler\n",
    "3. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
    "4. Define a **PairSelector** that takes embeddings and original labels and returns valid pairs within a minibatch\n",
    "5. Define **OnlineContrastiveLoss** that will use a *PairSelector* and compute *ContrastiveLoss* on such pairs\n",
    "6. Train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8f10b-e3f9-4ac1-8151-18e90f72f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = OnlineContrastiveLoss(margin, HardNegativePairSelector())\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bc2ad-3541-4643-a4b4-446ccacb109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a2e58-9cf8-4281-a8e8-ba71ccf964f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_ocl, train_labels_ocl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_ocl, train_labels_ocl)\n",
    "val_embeddings_ocl, val_labels_ocl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_ocl, val_labels_ocl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f930e13-e20f-417c-81f0-86bea550b868",
   "metadata": {},
   "source": [
    "## Online triplet selection\n",
    "## Steps\n",
    "1. Create **BalancedBatchSampler** - samples $N$ classes and $M$ samples *datasets.py*\n",
    "2. Create data loaders with the batch sampler\n",
    "3. Define **embedding** *(mapping)* network $f(x)$ - **EmbeddingNet** from *networks.py*\n",
    "4. Define a **TripletSelector** that takes embeddings and original labels and returns valid triplets within a minibatch\n",
    "5. Define **OnlineTripletLoss** that will use a *TripletSelector* and compute *TripletLoss* on such pairs\n",
    "6. Train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27b3e-3316-4dc4-bbf0-894644fcdbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create mini batches by sampling labels that will be present in the mini batch and number of examples from each class\n",
    "train_batch_sampler = BalancedBatchSampler(train_dataset.train_labels, n_classes=10, n_samples=25)\n",
    "test_batch_sampler = BalancedBatchSampler(test_dataset.test_labels, n_classes=10, n_samples=25)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "online_train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "online_test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_batch_sampler, **kwargs)\n",
    "\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = OnlineTripletLoss(margin, RandomNegativeTripletSelector(margin))\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a95e65-c92a-4112-98d1-799c7eb03ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fadcd-dc1f-46ab-8c62-ae0fcfbba5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_otl, train_labels_otl = extract_embeddings(train_loader, model)\n",
    "plot_embeddings(train_embeddings_otl, train_labels_otl)\n",
    "val_embeddings_otl, val_labels_otl = extract_embeddings(test_loader, model)\n",
    "plot_embeddings(val_embeddings_otl, val_labels_otl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0918a-6ea7-47fe-a096-2ee374dbd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_emb_online, display_emb, display_label_online, display_label = train_embeddings_ocl, train_embeddings_cl, train_labels_ocl, train_labels_cl\n",
    "# display_emb_online, display_emb, display_label_online, display_label = val_embeddings_ocl, val_embeddings_cl, val_labels_ocl, val_labels_cl\n",
    "\n",
    "x_lim = (np.min(display_emb_online[:,0]), np.max(display_emb_online[:,0]))\n",
    "y_lim = (np.min(display_emb_online[:,1]), np.max(display_emb_online[:,1]))\n",
    "x_lim = (min(x_lim[0], np.min(display_emb[:,0])), max(x_lim[1], np.max(display_emb[:,0])))\n",
    "y_lim = (min(y_lim[0], np.min(display_emb[:,1])), max(y_lim[1], np.max(display_emb[:,1])))\n",
    "\n",
    "plot_embeddings(display_emb, display_label, x_lim, y_lim)\n",
    "plot_embeddings(display_emb_online, display_label_online, x_lim, y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837adf9-cdbc-4957-9f12-a6251b2bf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_emb_online, display_emb, display_label_online, display_label = train_embeddings_otl, train_embeddings_tl, train_labels_otl, train_labels_tl\n",
    "# display_emb_online, display_emb, display_label_online, display_label = val_embeddings_otl, val_embeddings_tl, val_labels_otl, val_labels_tl\n",
    "x_lim = (np.min(display_emb_online[:,0]), np.max(display_emb_online[:,0]))\n",
    "y_lim = (np.min(display_emb_online[:,1]), np.max(display_emb_online[:,1]))\n",
    "x_lim = (min(x_lim[0], np.min(display_emb[:,0])), max(x_lim[1], np.max(display_emb[:,0])))\n",
    "y_lim = (min(y_lim[0], np.min(display_emb[:,1])), max(y_lim[1], np.max(display_emb[:,1])))\n",
    "plot_embeddings(display_emb, display_label, x_lim, y_lim)\n",
    "plot_embeddings(display_emb_online, display_label_online, x_lim, y_lim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myTorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
